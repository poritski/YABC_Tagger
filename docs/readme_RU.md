## Лемматизатор ЭКБЯ: общие сведения ##

В этом репозитории находится лемматизатор, созданный в 2013 г. в рамках проекта &laquo;Экспериментальный корпус белорусского языка&raquo; (<i>Yet Another Belorussian Corpus</i>, см. его [страницу](https://github.com/poritski/YABC)). Лемматизатор является правиловым, в его основу положен Грамматический словарь белорусского языка, тома которого доступны на сайте slounik.org ([существительное](http://www.slounik.org/nazounik), [глагол](http://www.slounik.org/dzsl), [остальные части речи](http://www.slounik.org/prym)).

## Подготовка к работе ##

Скачайте [ZIP-архив с файлами проекта](https://github.com/poritski/YABC_Tagger/archive/master.zip) и распакуйте его куда-либо на локальной машине. Чтобы использовать лемматизатор ЭКБЯ, нужен интерпретатор [Perl](http://www.activestate.com/activeperl/downloads) 5.x (лучше всего 5.10 или более новый) с установленными модулями [`Array::Utils`](http://search.cpan.org/dist/Array-Utils/Utils.pm), [`List::Util`](http://search.cpan.org/~pevans/Scalar-List-Utils-1.31/lib/List/Util.pm) и [`Benchmark`](http://search.cpan.org/~rjbs/perl-5.18.1/lib/Benchmark.pm). Программа разработана под ОС Windows, но её перенос на Unix-подобные ОС не должен вызывать затруднений.

## Параметры командной строки ##

Лемматизатор запускается из командной строки и принимает следующие параметры:

* `-i [путь к файлу]` – единственный обязательный параметр: входной текстовый файл на белорусском языке. Предполагается, что он токенизирован и описание каждого токена расположено на отдельной строке. Технически это должен быть TSV-файл в любое число колонок, последняя из которых содержит сам токен; вообще говоря, колонка может быть и одна. Для токенизации можно пользоваться, например, скриптом Г. Шмида из проекта [TreeTagger](http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger) с нашей обёрткой `preprocessor.pl`. Обе эти программы есть в репозитории.

* `-c [число]` – количество колонок во входном файле. По умолчанию оно принимается равным трём: идентификатор текстового фрагмента – порядковый номер токена в нём – сам токен.

* `-o [путь к файлу]` – выходной файл. По умолчанию результаты лемматизации выводятся в файл, имя которого отличается от имени входного файла префиксом `tagged_`. Это TSV-файл, колонок в котором на две больше, чем во входном (добавляются леммы и грамматические ярлыки).

* `-u [путь к файлу]` – файл с информацией о неопознанных токенах. По умолчанию эти сведения выводятся в файл, имя которого отличается от имени входного файла префиксом `unknown_`.

* `-m [bnkorpus|multext_east]` – маска тегсета. По умолчанию используется [тегсет ЭКБЯ](http://htmlpreview.github.io/?https://github.com/poritski/YABC_Tagger/blob/master/docs/tagset_YABC_RU.html). При включённой опции `-m bnkorpus` ярлыки возвращаются в формате [грамматической базы В. А. Кощенко](http://bnkorpus.info/download.html), впрочем, несколько адаптированном (см. [подробности](http://htmlpreview.github.io/?https://github.com/poritski/YABC_Tagger/blob/master/docs/tagset_bnkorpus_RU.html)). Когда будет реализовано отображение из тегсета ЭКБЯ в [MulTEXT East](http://nl.ijs.si/ME/Vault/V3/msd/html), включать соответствующую маску можно будет опцией `-m multext_east`.

## Образец применения ##

Покажем весь цикл подготовки и грамматической разметки текстов на примере, который пользователь может повторить своими руками. В папке `./raw/` содержатся две группы текстов: 100 статей из газеты &laquo;Звязда&raquo; (`./raw/zviazda_sample/`) и две повести Максима Горецкого (`./raw/Harecki/`). Их нужно токенизировать и лемматизовать.

1. Создадим файл `dirlist.txt`, в котором на отдельных строках содержатся имена поддиректорий без полных путей, т. е. просто `zviazda_sample` и `Harecki`. Создадим одноимённые поддиректории в папке `./tokenized/`. (Для большей наглядности этот шаг уже проделан.)
2. Запустим скрипт `preprocessor.pl`. В `./tokenized/zviazda_sample/` и `./tokenized/Harecki/` поступят токенизированные файлы.
3. Запустим скрипт `conjoiner.pl`. В основной рабочей папке появятся файлы `corpus.txt` и `corpus_fileid.txt`. Первый из них – это трёхколоночный TSV-файл, результат сцепления токенизированных текстов. Второй из них – это таблица соответствий между числовыми идентификаторами файлов и их первоначальными именами.
4. Запустим лемматизатор из командной строки:<br>
   <span style="padding-left: 3em">`perl tagger.pl -i corpus.txt`</span><br>
В рабочей папке появятся файлы `tagged_corpus.txt` (результат лемматизации) и `unknown_corpus.txt` (список неопознанных токенов). Консольный вывод будет выглядеть приблизительно так:<br>
<span style="padding-left: 3em">`Reading database...`</span><br>
<span style="padding-left: 3em">`Complete in 9.42 seconds.`</span><br>
<span style="padding-left: 3em">`Processing input...`</span><br>
<span style="padding-left: 3em">`Complete in 1.16 seconds.`</span><br>
<span style="padding-left: 3em">`110762 tokens in total, 106331 tokens processed (0.9600).`</span><br>
<span style="padding-left: 3em">`Recognized at once: 92044 (0.8310).`</span><br>
<span style="padding-left: 3em">`2nd level gain: 14175 (0.1280).`</span><br>
<span style="padding-left: 3em">`3rd level gain: 0 (0.0000).`</span><br>
<span style="padding-left: 3em">`4th level gain: 112 (0.0010).`</span><br>
<span style="padding-left: 3em">`Press any key to exit.`</span><br>
Отсюда видно, что входной корпус содержит около 110 тыс. токенов и что 96% из них распознано лемматизатором. Смысл остальных обозначений разъясняется в дальнейших параграфах.

## Некоторые детали реализации ##

Собственно лемматизатор состоит из файла данных `./base/db.txt` и скрипта `tagger.pl`. Скрипт восстанавливает из файла данных две хэш-таблицы: лемм и грамматических ярлыков. Механизм обработки входного текста предусматривает, что токен может обнаружиться в файле данных:

* Сразу, т. е. точно в таком виде, как в тексте.
* После технической нормализации: замены _ў_ на _у_ в начале слова и латинского _i_ на белорусское в подходящих контекстных условиях.
* После орфографической нормализации: удаления смягчающего _ь_ в контекстных условиях, которые предусматривает [&laquo;дзеясловіца&raquo;](https://be-x-old.wikipedia.org/wiki/Дзеясловіца). (Белорусская [классическая орфография](https://be-x-old.wikipedia.org/wiki/Беларускі_клясычны_правапіс) распознаётся лемматизатором лишь в этом объёме.)
* После удаления дефиса – знака переноса.
* После разбиения по дефису на две или три самостоятельные части (случай сложного существительного, двойного глагола и т. п.).
* В любом из предшествующих вариантов с приведением к нижнему регистру.

Кроме того, токен может попасть в один из классов, опознаваемых regex-шаблонами: слово латинскими буквами, русское слово, инициал имени, целое число в арабских или римских цифрах, десятичная дробь, время, дата, счёт матча, порядковый номер в нумерованном списке и проч.

С учётом того, насколько часто каждая из описанных ситуаций склонна возникать при грамматическом анализе художественных и газетных текстов, обработка разделена на четыре &laquo;уровня&raquo; или &laquo;шага&raquo;:

1. Токен в исходном виде (+ приведённый к нижнему регистру).
2. Технически нормализованный токен (+ приведённый к нижнему регистру). Шаблоны.
3. Орфографически нормализованный токен (+ приведённый к нижнему регистру).
4. Удаление дефиса или разбивка по нему.

Переход к каждому следующему шагу происходит, если на предыдущем токен не был опознан. Когда все возможности исчерпаны без результата, возвращаются грамматический тег `UNK` и лемма, равная исходному токену с прибавлением слева вопросительного знака.

Фрагмент консольного вывода, приведённый выше, показывает, что в 100 статьях &laquo;Звязды&raquo; и двух повестях Горецкого 83.1% токенов было опознано сразу, 12.8% – после технической нормализации или как удовлетворяющие шаблонам. Дальнейшие шаги почти не добавили производительности, потому что тексты набраны &laquo;наркомовкой&raquo; и содержат немного дефисных написаний.

Файл данных фиксирует некоторые орфографические варианты, возникшие после реформы 2008 г. (<i>пяцьдзесят</i> / <i>пяцьдзясят</i> и под.). В таких случаях используется помета `[ORTH]` или `[ORTH=нормативное написание]`, прибавляемая к лемме справа. &laquo;Дзеясловіцу&raquo; при различии с официальной нормой лемматизатор маркирует пометой `[TAR]`.

## Быстродействие и качество разбора ##

Лемматизатор ЭКБЯ в его нынешнем виде удобен для анализа больших текстовых коллекций (до десятков млн токенов), сцеплённых в один файл. Применять его к маленьким документам невыгодно потому, что программа загружает файл данных `db.txt` в память каждый раз при запуске. На ноутбуке одного из разработчиков (2 ГГц CPU, 3 Гб RAM, WinXP SP3) это занимает около 9.5 секунд; при маскировке тегсета время ещё увеличивается.

Скорость выполнения грамматического разбора зависит от свойств входного текста (качество набора и / или вычитки, доля &laquo;дзеясловічных&raquo; написаний и проч.). Коллекции текстов, подготовленные и обследованные нами, анализировались на скорости около 100 тыс. токенов в секунду. Подробную статистику можно видеть в следующей таблице:

<center>
<table>
<tr align="center"><td>Подкорпус</td><td>Время</td><td>Всего токенов</td><td>Распознано</td><td>Шаг 1</td><td>Шаг 2</td><td>Шаг 3</td><td>Шаг 4</td></tr>
<tr><td>&laquo;Голас Радзімы&raquo;</td><td>4.05</td><td>417550</td><td>406725 = 97.41%</td><td>384707</td><td>20995</td><td>11</td><td>1012</td></tr>
<tr><td>&laquo;Чырвоная змена&raquo;</td><td>3.87</td><td>375269</td><td>368920 = 98.38%</td><td>278470</td><td>90207</td><td>0</td><td>243</td></tr>
<tr><td>&laquo;Звязда&raquo;</td><td>28.19</td><td>2481775</td><td>2432989 = 98.03%</td><td>1792116</td><td>639152</td><td>7</td><td>1714</td></tr>
<tr><td>&laquo;Маладосць&raquo;</td><td>8.84</td><td>1022215</td><td>1004368 = 98.25%</td><td>976369</td><td>27029</td><td>0</td><td>970</td></tr>
<tr><td>&laquo;Полымя&raquo;</td><td>9.66</td><td>1059707</td><td>1038559 = 98.00%</td><td>1007046</td><td>30457</td><td>2</td><td>1054</td></tr>
<tr><td>&laquo;Дзеяслоў&raquo;</td><td>7.98</td><td>790798</td><td>768107 = 97.13%</td><td>729993</td><td>25286</td><td>11697</td><td>1131</td></tr>
<tr><td>Старая проза</td><td>6.48</td><td>728462</td><td>712743 = 97.84%</td><td>686082</td><td>25957</td><td>1</td><td>703</td></tr>
</table>
</center>

Как видно, медленнее всего анализировались статьи из газеты &laquo;Звязда&raquo; и приложения к ней &laquo;Чырвоная змена&raquo; (88 и 97 тыс. токенов в секунду). Это объясняется низким качеством набора – неуместным употреблением латинского _i_, ср. долю распознаний на втором шаге: 25.7% и 24.0% при менее чем 5% во всех остальных подкорпусах. Сравнительно медленно (99 тыс. токенов в секунду) анализировались публикации журнала &laquo;Дзеяслоў&raquo;, что связано с особенностями орфографии, ср. долю распознаний на третьем шаге: 1.5% при исчезающе малых долях во всех остальных подкорпусах.

Охват текстового материала в обследованных коллекциях в среднем не ниже 97%. В отдельных фрагментах качество может быть и хуже за счёт обилия собственных имён, отсутствующих в базе, передачи иноязычной или диалектной речи и т. п.

## Условия использования ##

Лемматизатор распространяется под BSD-подобной лицензией из 2 пунктов, см. её текст [по-русски](license_RU.md) или [по-английски](license_EN.md). При использовании программы желательно (хотя и не обязательно) ссылаться на сайт [slounik.org](http://www.slounik.org), разработчики которого опубликовали в сети Грамматический словарь белорусского языка, и на [страницу](https://github.com/poritski/YABC_Tagger) нашего проекта.