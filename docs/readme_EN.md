## YABC Tagger: general info ##This repository contains a POS tagger of Belorussian. The tool has been implemented in 2013 and is part of "Yet Another Belorussian Corpus" project (YABC, see its [Github page](https://github.com/poritski/YABC)). The tagger is rule-based; _Grammatical Dictionary of Belorussian_, available at slounik.org ([noun](http://www.slounik.org/nazounik), [verb](http://www.slounik.org/dzsl), [other parts of speech](http://www.slounik.org/prym)), constitutes the core of its database.## Setup ##Download the [zip archive](https://github.com/poritski/YABC_Tagger/archive/master.zip) of this repo and unpack it anywhere on the local machine. To run YABC tagger, you need [Perl](http://www.activestate.com/activeperl/downloads) 5.x (5.10 or later is best) with packages [`Array::Utils`](http://search.cpan.org/dist/Array-Utils/Utils.pm), [`List::Util`](http://search.cpan.org/~pevans/Scalar-List-Utils-1.31/lib/List/Util.pm) and [`Benchmark`](http://search.cpan.org/~rjbs/perl-5.18.1/lib/Benchmark.pm) installed. Although the program has been designed to run on Windows, it shouldn't take much effort to adjust it to Unix-like systems.## Command line options ##The tagger is run in the command line and accepts the following options:* `-i [path to file]` Ц input (the only required parameter). The input text file is assumed to be tokenized, and each token description is to be located at a separate line. Technically, the input must be TSV with any number of columns, provided that the last column contains tokens themselves; in particular, there might be only one column. You may use for tokenization the script by Helmut Schmid originally written for [TreeTagger](http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger), wrapped around with our script `preprocessor.pl` (check out both programs in the repo).* `-c [integer]` Ц number of columns in the input file. By default there are three columns: the first for text chunk identifiers, the second for token numbers within the chunks, and the third for tokens themselves.* `-o [path to file]` Ц output. By default the file is created in the same directory as the input file, and its name has prefix `tagged_`. Output is TSV with two more columns than input, one of them containing lemmata, the other grammatical tags.* `-u [path to file]` Ц statistics of unknown tokens. By default the file is created in the same directory as the input file, and its name has prefix `unknown_`.* `-m [bnkorpus|multext_east]` Ц tagset mask. By default the tagger uses YABC tagset (see description in [Russian](http://htmlpreview.github.io/?https://github.com/poritski/YABC_Tagger/blob/master/docs/tagset_YABC_RU.html) or [Belorussian](http://htmlpreview.github.io/?https://github.com/poritski/YABC_Tagger/blob/master/docs/tagset_YABC_BE.html)). When `-m bnkorpus` option is passed, the tags are mapped into [Uladzimir Koshchanka's grammatical database](http://bnkorpus.info/download.html) tagset, though slightly modified (see details in [Russian](http://htmlpreview.github.io/?https://github.com/poritski/YABC_Tagger/blob/master/docs/tagset_bnkorpus_RU.html) or [Belorussian](http://htmlpreview.github.io/?https://github.com/poritski/YABC_Tagger/blob/master/docs/tagset_bnkorpus_BE.html)). Currently the mapping from YABC tagset to [MulTEXT East](http://nl.ijs.si/ME/Vault/V3/msd/html) is not implemented. As soon as it is, the respective tagset mask will be available as `-m multext_east`.## Using the tagger: A hands-on example ##In this section the user is welcome to follow a step-by-step tutorial explaining how to proceed from raw to tagged texts. In the `./raw/` directory there are two groups of texts: 100 articles published by "Zviazda", a Belorussian newspaper (`./raw/zviazda_sample/`), and two stories by the prominent pre-war writer Maksim Harecki (`./raw/Harecki/`). The task is to tokenize and tag them.1. Create file `dirlist.txt` and write down in it the source subdirectory names (without full paths, i.e. `zviazda_sample` and `Harecki`) on separate lines. Create the same name subdirectories in `./tokenized/`. (For your convenience this has been already done.)2. Run `preprocessor.pl`. The tokenized files will be written into `./tokenized/zviazda_sample/` and `./tokenized/Harecki/`.3. Run `conjoiner.pl`. In the main directory you will get two files, `corpus.txt` which is a concatenation of the tokenized texts (three-column TSV, as described above), and `corpus_fileid.txt` which maps numeral file identifiers into their original names.4. Run YABC tagger in the command line:<br>   <span style="padding-left: 3em">`perl tagger.pl -i corpus.txt`</span><br>In the main directory you will get two files, `tagged_corpus.txt` which is the output and `unknown_corpus.txt` which is the statistics of unknown tokens. The console window is expected to look like this:<br><span style="padding-left: 3em">`Reading database...`</span><br><span style="padding-left: 3em">`Complete in 9.42 seconds.`</span><br><span style="padding-left: 3em">`Processing input...`</span><br><span style="padding-left: 3em">`Complete in 1.16 seconds.`</span><br><span style="padding-left: 3em">`110762 tokens in total, 106331 tokens processed (0.9600).`</span><br><span style="padding-left: 3em">`Recognized at once: 92044 (0.8310).`</span><br><span style="padding-left: 3em">`2nd level gain: 14175 (0.1280).`</span><br><span style="padding-left: 3em">`3rd level gain: 0 (0.0000).`</span><br><span style="padding-left: 3em">`4th level gain: 112 (0.0010).`</span><br><span style="padding-left: 3em">`Press any key to exit.`</span><br>So the input corpus is about 110 thousand tokens in size, and 96% of them have been recognized by the tagger. All the other designations are explained in the next two sections.## Some implementation details ##The tagger itself consists of the database file `./base/db.txt` and the script `tagger.pl`. The script recovers two hash tables with identical keys (wordforms), one for lemmata and the other for grammatical tags, from the database file. While the input text is being processed, there are several opportunities for each token to be recognized by the hashes:* Exactly as presented in the text.* With technical normalization applied, such as initial _Ґ_ &rarr; _у_ and Latin _i_ &rarr; Belorussian _≥_ in the suitable contexts.* With orthographic normalization applied, such as removal of _ь_ in ["dziejaslovitsa"](https://be-x-old.wikipedia.org/wiki/ƒзе€слов≥ца). (The [classical orthography](https://be-x-old.wikipedia.org/wiki/Ѕеларуск≥_кл€сычны_правап≥с) of Belorussian is currently handled only to this extent.)* With separating hyphen removed.* When split on hyphen (like compound nouns and some of the double verbs), which results in two or three disctinct wordforms.* Any of the previous alternatives in lowercase.Moreover the token may fall into one of regex-detected classes: word in Latin script, Russian word, first name initial, Roman numeral, integer or floating-point number, timestamp, game score, ordered list item etc.Given the relative frequency of each alternative within newspaper and fiction texts, the processing is divided into four "levels":1. Trying to recognize the token "as is" + lowercase.2. Technical normalization + lowercase. Regex matching.3. Orthographic normalization + lowercase.4. Trying to remove hyphen or split on it.The transition to each next level occurs if the token hasn't been recognized on the previous one. After trying all possibilities without satisfactory outcome, the tagger returns `UNK` for the grammatical tag and the token prefixed with `?` for the lemma.The console log presented above shows that in "Zviazda" sample and Maksim Harecki's texts 83.1% of all tokens have been recognized nearly as they are, and 12.8% either required technical normalization or matched handwritten regex templates. The further two levels of processing haven't brought any noticeable improvement, since the orthography is standard, and hyphenated spellings are quite rare.Some spelling variants which emerged in 2008, after the official recodification of Belorussian orthography (like _п€цьдзес€т_ / _п€цьдз€с€т_), are marked in the database file with `[ORTH]` or `[ORTH=standard spelling]` suffixes added to the lemmata. The tagger also marks "dziejaslovitsa"-compliant spellings with `[TAR]` suffix.## Performance ##YABC tagger as it is currently implemented performs well on large text collections (up to tens of millions of tokens) concatenated into single files. The reason not to run it on small documents is that the program reads the database file each time when it is started, which typically takes around 9.5 seconds on the laptop used for testing (2 GHz CPU, 3 Gb RAM, WinXP SP3). Tagset masking increases the cold-start time even further.The tagging speed hinges upon many properties of the input text, such as the quality of typesetting and proofreading, the percent of "dziejaslovitsa"-compliant spellings etc. Text collections prepared by ourselves were processed at the rate of around 100 thousand tokens/sec. Detailed statistics is provided in the following table:<center><table><tr align="center"><td> Subcorpus </td><td> Tagging time </td><td> Number of tokens </td><td> Recognized </td><td> Level 1 </td><td> Level 2 </td><td> Level 3 </td><td> Level 4 </td></tr><tr><td> "Holas Radzimy" </td><td> 4.05 </td><td> 417550 </td><td> 406725 = 97.41% </td><td> 384707 </td><td> 20995 </td><td> 11 </td><td> 1012 </td></tr><tr><td> "Chyrvonaja zmiena" </td><td> 3.87 </td><td> 375269 </td><td> 368920 = 98.38% </td><td> 278470 </td><td> 90207 </td><td> 0 </td><td> 243 </td></tr><tr><td> "Zviazda" </td><td> 28.19 </td><td> 2481775 </td><td> 2432989 = 98.03% </td><td> 1792116 </td><td> 639152 </td><td> 7 </td><td> 1714 </td></tr><tr><td> "Maladosc'" </td><td> 8.84 </td><td> 1022215 </td><td> 1004368 = 98.25% </td><td> 976369 </td><td> 27029 </td><td> 0 </td><td> 970 </td></tr><tr><td> "Polymia" </td><td> 9.66 </td><td> 1059707 </td><td> 1038559 = 98.00% </td><td> 1007046 </td><td> 30457 </td><td> 2 </td><td> 1054 </td></tr><tr><td> "Dziejaslou" </td><td> 7.98 </td><td> 790798 </td><td> 768107 = 97.13% </td><td> 729993 </td><td> 25286 </td><td> 11697 </td><td> 1131 </td></tr><tr><td> 1910Ц1950 fiction </td><td> 6.48 </td><td> 728462 </td><td> 712743 = 97.84% </td><td> 686082 </td><td> 25957 </td><td> 1 </td><td> 703 </td></tr></table></center>As the table shows, "Zviazda" and its supplement "Chyrvonaja zmiena" were the slowest to analyze (88 and 97 thousand tokens/sec respectively). This is due to low-quality typesetting with pervasive Latin _i_ instead of Belorussian, cf. level 2 recognition percent: 25.7% and 24.0%, while in the other subcorpora it is less than 5%. "Dziejaslou" was rather slow to analyze (99 thousand tokens/sec) because of its orthography, cf. level 3 recognition percent: 1.5%, while in the other subcorpora it is at the verge of statistical significance.The tagger covers no less than 97% of all tokens on average. Locally, the percentage of recognition might be sufficiently lower because of abundant proper names which are absent in the database, dialectal wordforms, transliterated words in a foreign language etc.## Terms of use ##The tagger is distributed under a two-clause BSD license, see its text [in English](license_EN.md) or [in Russian](license_RU.md). Beyond the license, the YABC tagger users are encouraged, though in no way required, to acknowledge [slounik.org](http://www.slounik.org) for publishing the _Grammatical Dictionary_ on the Web and our [project page](https://github.com/poritski/YABC_Tagger) for transforming it into a software application.